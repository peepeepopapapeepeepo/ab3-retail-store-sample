name: 03 - Let Orders use Aurora for PostgreSQL
on:
  workflow_dispatch:
env:
  AWS_REGION: "ap-southeast-1"
  AWS_EKS_NAME: "eks-ab3-apse1-poc-001"
  AWS_ROLE_ARN: "arn:aws:iam::711387113538:role/AB3-GitHub-PipelineAccess"
permissions:
  id-token: write
  contents: read
jobs:
  DeployAppToEKS:
    runs-on: [self-hosted, linux, ab3]
    environment: poc
    steps:
      - name: Git clone the repository
        uses: actions/checkout@v4
      - name: Configure aws credentials
        uses: aws-actions/configure-aws-credentials@v4.1.0
        with:
          role-to-assume: ${{ env.AWS_ROLE_ARN }}
          role-session-name: ab3-retail-store-sample
          aws-region: ${{ env.AWS_REGION }}
      - name: Update kube config
        run: aws eks update-kubeconfig --name ${{ env.AWS_EKS_NAME }} --region ${{ env.AWS_REGION }}
      - name: Deploy app
        continue-on-error: true
        env:
          APP: ${{ github.event.inputs.app }}
          MANIFEST: ${{ github.event.inputs.manifest }}
        run: |
          echo -e "\n ===> Deploy <=== "
          kubectl apply -f deploy/orders-overrided/

          echo -e "\n ===>  Restart Pod <=== "
          kubectl -n orders rollout restart deployment orders
          
          echo -e "\n ===>  Delete Local PostgreSQL <=== "
          kubectl -n orders delete sts orders-postgresql
          kubectl -n orders delete svc orders-postgresql
          kubectl -n orders delete secret orders-db

          echo -e "\n ===> Show New Configure <=== "
          kubectl -n checkout wait --for=condition=Ready --timeout=30s pods -l app.kuberneres.io/owner=retail-store-sample
          kubectl -n orders get cm orders -o yaml
          kubectl -n orders get secretproviderclass -o yaml
          kubectl -n orders get po -o yaml